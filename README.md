# Fiber Photometry processing pipeline
This is a pipeline in development to process Fiber Photometry data adapted to a fiber acquisition standard defined here: [Fiber Photometry Acquisition Standard](https://github.com/AllenNeuralDynamics/aind-file-standards/blob/main/docs/file_formats/fip.md).

The [fiber photometry pipeline](https://codeocean.allenneuraldynamics.org/capsule/0885100/tree) runs on [Nextflow](https://www.nextflow.io/) and contains the following steps:

* [aind-fip-nwb-base-capsule](https://github.com/AllenNeuralDynamics/aind-fip-nwb-base-capsule-standalone): FiberPhotometry Capsule which appends to an NWB subject file and packages raw timeseries data.

* [aind-fip-dff](https://github.com/AllenNeuralDynamics/aind-fip-dff): Processes input NWB files containing raw fiber photometry data by generating baseline-corrected (Î”F/F) and motion-corrected traces, which are then appended back to the NWB file.

* [aind-fip-qc-raw](https://github.com/AllenNeuralDynamics/aind-fip-qc-raw): QC capsule for fiber photometry. Currently in development for new standard.

* [aind-generic-quality-control-evaluation-aggregator](https://github.com/AllenNeuralDynamics/aind-generic-quality-control-evaluation-aggregator): Combines QC outputs into one QC JSON

# Input

Currently, the pipeline supports the following input data types:

* `aind`: data ingestion used at AIND. If an "fib" folder is included, fiber data will be packaged. The root directory must contain JSON files following [aind-data-schema](https://github.com/AllenNeuralDynamics/aind-data-schema).

Under the `fib` folder, the data will be stored as defined in the file standard linked above.

```plaintext
ðŸ“¦data
 â”£ ðŸ“‚MouseID_YYYY-MM-DD_HH-M-S
 â”ƒ â”£ ðŸ“‚fib
 â”£ ðŸ“œdata_description.json
 â”£ ðŸ“œsession.json
 â”— ðŸ“œprocessing.json
 ```
# Output

Tools used to read files in python are [h5py](https://pypi.org/project/h5py/), json and csv.

* `aind`: The pipeline outputs are saved under the `results` top-level folder with JSON files following [aind-data-schema](https://github.com/AllenNeuralDynamics/aind-data-schema). Each field of view (plane) runs as a parallel process from motion-correction to event detection. The first subdirectory under `results` is named according to Allen Institute for Neural Dynamics standard for derived asset formatting. Below that folder, each field of view is named according to the anatomical region of imaging and the index (or plane number) it corresponds to. The index number is generated before processing in the session.json which details out the imaging configuration during acquisition. As the movies go through the processsing pipeline, a JSON file called processing.json is created where processing data from input parameters are appended. The final JSON will sit at the root of the `results` folder at the end of processing. 

```plaintext
ðŸ“¦results
 â”ƒ ðŸ“‚dff-qc
 â”ƒ ðŸ“‚qc-raw
 â”ƒ ðŸ“‚nwb
 â”— ðŸ“œprocessing.json
 ```

The following files will be under the 'dff-qc' directory within the `results` folder (if there is fiber data to qc):

**`dff-qc`**

```plaintext
ðŸ“¦dff-qc
 â”£ ðŸ“œROI0_bright.png
 â”£ ðŸ“œROI0_exp.png
 â”£ ðŸ“œROI0_poly.png
 â”£ ðŸ“œROI1_bright.png
 â”£ ðŸ“œROI1_exp.png
 â”£ ðŸ“œROI1_poly.png
 â”£ ðŸ“œROI2_bright.png
 â”£ ðŸ“œROI2_exp.png
 â”£ ðŸ“œROI2_poly.png
 â”£ ðŸ“œROI3_bright.png
 â”£ ðŸ“œROI3_exp.png
 â”£ ðŸ“œROI3_poly.png
 ```
*Note: Prior to pipeline version 7, these files are indexed starting from 1, rather than 0. The version of the pipeline used to process each asset is present in the processing.json*

The following files will be under the 'qc-raw' directory within the `results` folder (if there is fiber data to qc):

**`qc-raw`**

```plaintext
ðŸ“¦qc-raw
â”£ ðŸ“œCMOS_Floor.pdf
â”£ ðŸ“œCMOS_Floor.png
â”£ ðŸ“œraw_traces.pdf
â”£ ðŸ“œraw_traces.png
```

**`nwb`**

The NWB output has both raw and processed data. The raw data can be found under the `acquisition` field in the NWB. It contains timerseries for each channel-fiber. Under the `processing` field, there will be a `fiber_photometry` module that will contain timeseries for different combinations of channel-fiber connection, dff, and motion-correction.

```plaintext
ðŸ“¦nwb
â”£ ðŸ“œacquisition
â”ƒ â”£ ðŸ“‚G_0
â”ƒ â”£ ðŸ“‚G_1
â”ƒ â”£ ðŸ“‚G_2
â”ƒ â”£ ðŸ“‚G_3
â”ƒ â”£ ðŸ“‚Iso_0
â”ƒ â”£ ðŸ“‚Iso_1
â”ƒ â”£ ðŸ“‚Iso_2
â”ƒ â”£ ðŸ“‚Iso_3
â”ƒ â”£ ðŸ“‚Red_0
â”ƒ â”£ ðŸ“‚Red_1
â”ƒ â”£ ðŸ“‚Red_2
â”ƒ â”£ ðŸ“‚Red_3
â”£ ðŸ“œprocessing
â”ƒ â”£ ðŸ“‚fiber_photometry
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚G_0_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚G_1_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚G_2_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚G_3_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚Iso_0_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚Iso_1_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚Iso_2_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚Iso_3_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚R_0_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚R_1_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚R_2_dff-poly_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-bright
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-bright_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-exp
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-exp_mc-ISO-IRLS
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-poly
â”ƒ â”ƒ â”£ ðŸ“‚R_3_dff-poly_mc-ISO-IRLS
```

# Parameters

No parameters are used for this pipeline

# Run

`aind` Runs in the Code Ocean pipeline [here](https://codeocean.allenneuraldynamics.org/capsule/7026342/tree). If a user has credentials for `aind` Code Ocean, the pipeline can be run using the [Code Ocean API](https://github.com/codeocean/codeocean-sdk-python). 

Derived from the example on the [Code Ocean API Github](https://github.com/codeocean/codeocean-sdk-python/blob/main/examples/run_pipeline.py)

```python
import os

from codeocean import CodeOcean
from codeocean.computation import RunParams
from codeocean.data_asset import (
    DataAssetParams,
    DataAssetsRunParam,
    PipelineProcessParams,
    Source,
    ComputationSource,
    Target,
    AWSS3Target,
)

# Create the client using your domain and API token.

client = CodeOcean(domain=os.environ["CODEOCEAN_URL"], token=os.environ["API_TOKEN"])

# Run a pipeline with ordered parameters.

run_params = RunParams(
    pipeline_id=os.environ["PIPELINE_ID"],
    data_assets=[
        DataAssetsRunParam(
            id="eeefcc52-b445-4e3c-80c5-0e65526cd712",
            mount="FIP",
        ),
)

computation = client.computations.run_capsule(run_params)

# Wait for pipeline to finish.

computation = client.computations.wait_until_completed(computation)

# Create an external (S3) data asset from computation results.

data_asset_params = DataAssetParams(
    name="My External Result",
    description="Computation result",
    mount="my-result",
    tags=["my", "external", "result"],
    source=Source(
        computation=ComputationSource(
            id=computation.id,
        ),
    ),
    target=Target(
        aws=AWSS3Target(
            bucket=os.environ["EXTERNAL_S3_BUCKET"],
            prefix=os.environ.get("EXTERNAL_S3_BUCKET_PREFIX"),
        ),
    ),
)

data_asset = client.data_assets.create_data_asset(data_asset_params)

data_asset = client.data_assets.wait_until_ready(data_asset)
```


